---
title: "p8105_hw2_zw2709"
author: zw2709
output: github_document
---

This is the Solution to p8105 Homework 2.


```{r setup}
library(tidyverse)
library(readxl)

```

## Problem 1

First define a path to dataset
```{r}
path_to_data1 = './data/Trash-Wheel-Collection-Totals-8-6-19.xlsx'
```

Read the Mr. Trashwheel dataset.

```{r}
trashwheel_df = 
  read_xlsx(
    path = path_to_data1,
    range = cell_cols('A:N')) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls), 
    sports_balls = as.integer(sports_balls)
  )
```


Read precipitation data for 2018 and 2017.

```{r}
precip_2018 = 
  read_excel(
    './data/Trash-Wheel-Collection-Totals-8-6-19.xlsx',
    sheet = '2018 Precipitation',
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)
```

```{r}
precip_2017 = 
  read_excel(
    './data/Trash-Wheel-Collection-Totals-8-6-19.xlsx',
    sheet = '2018 Precipitation',
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Now combine annual precipitation.

```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )
```

```{r}
precip_df = 
  bind_rows(precip_2018, precip_2017)
  
```

```{r}
left_join(precip_df, month_df, by = 'month')
```


This dataset contains information from the Mr. Trashwheel trash collector in Baltimore, Maryland. As trash enters the inner harbor, the trashwheel collects that trash, and stores it in a dumpster. The dataset contains information on year, month, and trash collected, including some specific kinds of trash. There are a total of `r nrow(trashwheel_df)` rows in our final dataset. Additional data sheets include month precipitation data.

The median number of sports balls found in a dumpster in 2017 was `r trashwheel_df %>% filter(year == 2017) %>% pull(sports_balls) %>% median()`

The total precipitation in 2018 was `r precip_df %>% filter(year == 2018) %>% pull(total) %>% sum()` inches.






## Problem 2


Read the csv file and retain some columns.

```{r}
NYC_data = read_csv('./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv')
janitor::clean_names(NYC_data)

NYC_data_df = 
  select(NYC_data, Line:Entry, Vending, ADA)
```



Convert the entry variable from character to logical variable.
```{r}
NYC_df =
  mutate(
    NYC_data_df, Entry = recode(Entry, 'YES' = 'TRUE', 'NO' = 'FALSE'),
    Entry = as.logical(Entry)
  ) %>% 
  janitor::clean_names()


```


This dataset contains information about each subway station in NYC, including the these variables: station name, station longitute, station latitude, the routes served, the entry, entrance type, line information and ADA compliance. 

So far, all the variables mentioned above are selected from the csv file 'NYC_Transit_Subway_Entrance_And_Exit_Data.csv' and we have changed the entry variable type from character(YES/NO) to logical variable(True/False).

The resulting dataset has `r nrow(NYC_df)` rows and `r ncol(NYC_df)` columns in total, and the dimension is `r nrow(NYC_df)` * `r ncol(NYC_df)`.

Overall the data is relatively tidy, but some terms with NA seems to be handled later.




ANswer questions

* How many distinct stations? 
Since the stations are identified by both name and line, so use distinct function
```{r}
dist_sta = distinct(NYC_df, line, station_name, keep_all = TRUE)
```
SO there are `r nrow(dist_sta)` distinct stations.




```{r}
dist_ada = 
  filter(NYC_df, ada == TRUE) %>% 
  distinct(station_name, line)
  
```
Among these distinct stations, there are `r nrow(dist_ada)` stations with ADA compliance.



```{r}
NYC_df_new=

  mutate(
    NYC_data_df, Vending = recode(Vending, 'YES' = 'TRUE', 'NO' = 'FALSE'),
    Vending = as.logical(Vending)
  ) %>% 
  janitor::clean_names()



dist_ved = 
  filter(NYC_df_new, vending == FALSE) 


```
Among these distinct stations, there are `r nrow(dist_ved)` stations without vending allow entrance, so the proportion is 183/465=39%




Then we need to reformat data.
```{r}
reformat = 
  NYC_df_new %>% 
  mutate(route8 = as.character(route8), route9 = as.character(route9), route10 = as.character(route10), route11 = as.character(route11)) %>% 
  drop_na(route1:route11) %>% 
  pivot_longer(
    route1:route11,
    names_to = 'route_name',
    values_to = 'route_number'
  )
```

How many distinct stations serve the A train?
```{r}
A_train = 
  filter(reformat, route_number == 'A') %>% 
  distinct(station_name, line)
```
There are `r nrow(A_train)` distinct stations serve the A train.


```{r}
A_train_ada = 
  filter(reformat, route_number == 'A') %>% 
  filter(ada == TRUE) %>% 
  distinct(station_name, line)
 

```
Among that serving A stations, `r nrow(A_train_ada)` are ADA compliant.







## Problem 3

First, clean the data in pols-month.csv

```{r}
pol = 
  read_csv('./data/pols-month.csv') %>% 
  janitor:: clean_names() %>% 
  separate(mon, into = c('year', 'month', 'day')) %>%      #break up the mon variable
  mutate(month, month = as.integer(month))

```




Replace month name with month number

```{r}

month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

pol_d = 
  left_join(pol, month_df, by = 'month') %>% 
  select(-month) %>% 
  relocate(year, month_name)

```



Create the president variable, remove day variable

```{r}
pol_df =
  mutate(pol_d, president = gov_gop + sen_gop + rep_gop + gov_dem + sen_dem + rep_dem) %>% 
  select(-day)
   
```




Second, clean the data in snp.csv
```{r}
snp = 
  read_csv('./data/snp.csv') %>% 
  janitor:: clean_names() %>% 
  separate(date, into = c('month', 'day', 'year')) %>% 
  
  mutate(month, month = as.integer(month)) 
  
```
Replace month name with month number
```{r}
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name
  )

snp_df = 
  left_join(snp, month_df, by = 'month') %>% 
  arrange(year, month) %>% 
  select(-month) %>% 
  select(-day) %>% 
  relocate(year, month_name)
```



Third, tidy the unemployment data

```{r}
une_df = 
  read_csv('./data/unemployment.csv') %>% 
  janitor:: clean_names() %>% 
  
  pivot_longer(
    jan:dec,
    names_to = 'month_name',
    values_to = 'unemployment'
  ) %>% 
  
  mutate(year, year = as.character(year))

```


THEN merge into one data frame
```{r}
res = left_join(pol_df, snp_df, by = c('month_name', 'year'))
result = left_join(res, une_df, by = c('month_name', 'year'))


```

There are three datasets in total: pols, snp, unemployment with corresponding data frame called pol_df, snp_df and une_df.


The pols dataset contains 822 observations of the date information and other 8 variables related to the number of national politicians who are democratic or republican at any given time, with one more created variable called 'president'.

The snp dataset contains 787 observations of the date information and the closing value of Standard & Poorâ€™s stock market index (S&P) on the associated date.

The unemployment dataset contains 68 observations about percentage of unemployment and the month associated.


Since these three datasets have elements of years and months in common, with data wrangling process, they could be merged into one dataframe called 'result'.

The result dataset has `r nrow(result)` rows and `r ncol(result)` columns, and the dimension is `r nrow(result)` * `r ncol(result)`.

The range of the years is :`r range(pull(result, year))`.

The name of the key variables are: `r names(result)`.



















